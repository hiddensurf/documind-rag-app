"""
Multi-Model CAD Analyzer
- Primary: Google Studio Gemini API (direct)
- Fallback: OpenRouter Gemini (when quota exceeded)
- Additional: OpenRouter models (DeepSeek, NVIDIA, Qwen)
"""

import os
import io
import base64
import httpx
import logging
from typing import Dict, Optional
from pathlib import Path
from PIL import Image
import google.generativeai as genai

logger = logging.getLogger(__name__)

class MultiModelCADAnalyzer:
    """Advanced CAD analyzer with dual Gemini sources + OpenRouter models"""
    
    MODELS = {
        "gemini-2.5-flash": {
            "name": "Gemini 2.5 Flash",
            "provider": "GEMINI_DIRECT",
            "fallback_provider": "OPENROUTER",
            "fallback_model": "google/gemini-2.0-flash-exp:free",
            "capabilities": ["vision", "fast"],
            "context_window": "1M tokens",
            "free": True,
            "notes": "Google AI Studio (with OpenRouter fallback)"
        },
        "google/gemini-2.0-flash-exp:free": {
            "name": "Gemini 2.0 Flash (OpenRouter Backup)",
            "provider": "OPENROUTER",
            "capabilities": ["vision", "fast"],
            "context_window": "1M tokens",
            "free": True,
            "notes": "OpenRouter fallback for quota issues"
        },
        "deepseek/deepseek-r1": {
            "name": "DeepSeek R1",
            "provider": "OPENROUTER",
            "capabilities": ["reasoning", "advanced"],
            "context_window": "64K tokens",
            "free": False,
            "notes": "Excellent reasoning, chain-of-thought"
        },
        "nvidia/nemotron-nano-12b-v2-vl:free": {
            "name": "NVIDIA Nemotron Nano VL",
            "provider": "OPENROUTER",
            "capabilities": ["vision", "technical"],
            "context_window": "32K tokens",
            "free": True,
            "notes": "Optimized for technical diagrams"
        },
        "qwen/qwen3-235b-a22b": {
            "name": "Qwen 3 235B",
            "provider": "OPENROUTER",
            "capabilities": ["reasoning", "advanced"],
            "context_window": "32K tokens",
            "free": False,
            "notes": "Large reasoning model"
        }
    }
    
    def __init__(self, gemini_api_key: str = None, openrouter_api_key: str = None):
        """Initialize with both API keys"""
        self.gemini_api_key = gemini_api_key or os.getenv('GOOGLE_API_KEY')
        self.openrouter_api_key = openrouter_api_key or os.getenv('OPENROUTER_API_KEY')
        
        # Setup Google Studio Gemini (primary)
        if self.gemini_api_key:
            try:
                genai.configure(api_key=self.gemini_api_key)
                self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
                logger.info("âœ… Google Studio Gemini initialized (primary)")
            except Exception as e:
                logger.warning(f"âš ï¸ Google Studio Gemini init failed: {e}")
                self.gemini_model = None
        else:
            logger.warning("âš ï¸ No GOOGLE_API_KEY - direct Gemini unavailable")
            self.gemini_model = None
        
        # Setup OpenRouter (fallback + other models)
        if not self.openrouter_api_key:
            logger.warning("âš ï¸ No OPENROUTER_API_KEY - fallback unavailable")
    
    async def analyze_with_gemini_direct(
        self,
        image_bytes: Optional[bytes],
        prompt: str
    ) -> str:
        """Analyze with Google Studio Gemini (PRIMARY)"""
        if not self.gemini_model:
            raise Exception("Google Studio Gemini not initialized")
        
        try:
            if image_bytes:
                image = Image.open(io.BytesIO(image_bytes))
                response = self.gemini_model.generate_content([prompt, image])
            else:
                response = self.gemini_model.generate_content(prompt)
            return response.text
        except Exception as e:
            error_msg = str(e)
            # Check if it's a quota error
            if "429" in error_msg or "quota" in error_msg.lower():
                raise QuotaExceededError(f"Gemini quota exceeded: {error_msg}")
            raise Exception(f"Gemini API error: {error_msg}")
    
    async def analyze_with_openrouter(
        self,
        image_bytes: Optional[bytes],
        prompt: str,
        model_id: str
    ) -> str:
        """Analyze with OpenRouter (FALLBACK + other models)"""
        try:
            url = "https://openrouter.ai/api/v1/chat/completions"
            
            headers = {
                "Authorization": f"Bearer {self.openrouter_api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://documind.app",
                "X-Title": "DocuMind CAD Analyzer"
            }
            
            if image_bytes:
                base64_image = base64.b64encode(image_bytes).decode('utf-8')
                message_content = [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
                ]
            else:
                message_content = prompt
            
            payload = {
                "model": model_id,
                "messages": [{"role": "user", "content": message_content}]
            }
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(url, headers=headers, json=payload)
                response.raise_for_status()
                return response.json()['choices'][0]['message']['content']
                
        except httpx.HTTPStatusError as e:
            raise Exception(f"OpenRouter error ({e.response.status_code}): {e.response.text}")
        except Exception as e:
            raise Exception(f"OpenRouter failed: {e}")
    
    async def analyze_with_auto_fallback(
        self,
        image_bytes: Optional[bytes],
        prompt: str,
        model_id: str = "gemini-2.5-flash"
    ) -> tuple[str, str]:
        """
        Smart analysis with automatic fallback
        Returns: (response_text, provider_used)
        """
        model_info = self.MODELS.get(model_id)
        if not model_info:
            raise ValueError(f"Unknown model: {model_id}")
        
        # If it's the primary Gemini model, try direct first, then fallback
        if model_info['provider'] == 'GEMINI_DIRECT':
            # Try Google Studio first
            try:
                logger.info("ðŸ”µ Trying Google Studio Gemini (primary)...")
                response = await self.analyze_with_gemini_direct(image_bytes, prompt)
                logger.info("âœ… Google Studio Gemini succeeded")
                return response, "Google Studio Gemini"
            except QuotaExceededError as e:
                # Quota exceeded - use OpenRouter fallback
                logger.warning(f"âš ï¸ Google Studio quota exceeded, switching to OpenRouter fallback...")
                fallback_model = model_info.get('fallback_model')
                if not fallback_model or not self.openrouter_api_key:
                    raise Exception("Quota exceeded and no fallback available")
                
                response = await self.analyze_with_openrouter(image_bytes, prompt, fallback_model)
                logger.info("âœ… OpenRouter fallback succeeded")
                return response, "OpenRouter Gemini (fallback)"
            except Exception as e:
                # Other error - try fallback anyway
                logger.error(f"âŒ Google Studio error: {e}")
                fallback_model = model_info.get('fallback_model')
                if fallback_model and self.openrouter_api_key:
                    logger.info("ðŸ”„ Attempting OpenRouter fallback...")
                    response = await self.analyze_with_openrouter(image_bytes, prompt, fallback_model)
                    return response, "OpenRouter Gemini (fallback)"
                raise
        
        # For other models, use OpenRouter directly
        else:
            response = await self.analyze_with_openrouter(image_bytes, prompt, model_id)
            return response, model_info['name']
    
    async def comprehensive_analysis(
        self,
        png_path: str,
        model_id: str = "gemini-2.5-flash"
    ) -> Dict:
        """
        Run comprehensive 5-stage CAD analysis with automatic fallback
        """
        if not Path(png_path).exists():
            raise FileNotFoundError(f"PNG file not found: {png_path}")
        
        model_info = self.MODELS.get(model_id)
        if not model_info:
            raise ValueError(f"Unknown model: {model_id}")
        
        if 'vision' not in model_info['capabilities']:
            raise ValueError(f"Model {model_id} doesn't support vision")
        
        logger.info(f"ðŸ” 5-stage analysis with {model_info['name']}...")
        
        # Read image
        with open(png_path, 'rb') as f:
            image_bytes = f.read()
        
        # Track which provider was actually used
        provider_used = None
        
        # Define stages
        stages = {
            "stage_1_overview": "Analyze this CAD: 1) type 2) purpose 3) complexity 4) key features",
            "stage_2_technical": "Technical aspects: 1) dimensions 2) standards 3) annotations 4) units",
            "stage_3_components": "Components: 1) major parts 2) relationships 3) materials 4) features",
            "stage_4_measurements": "Measurements: 1) critical dims 2) tolerances 3) angles 4) constraints",
            "stage_5_quality": "Quality: 1) clarity 2) completeness 3) issues 4) recommendations"
        }
        
        # Run stages
        results = {}
        for i, (stage_name, prompt) in enumerate(stages.items(), 1):
            logger.info(f"  Stage {i}/5: {stage_name.replace('_', ' ').title()}...")
            response, used_provider = await self.analyze_with_auto_fallback(
                image_bytes, 
                prompt, 
                model_id
            )
            results[stage_name] = response
            if not provider_used:
                provider_used = used_provider
        
        # Executive summary (text-only)
        logger.info("  Executive summary...")
        summary_prompt = f"""Summarize in 2-3 sentences:
Overview: {results['stage_1_overview'][:200]}
Technical: {results['stage_2_technical'][:200]}"""
        
        summary, _ = await self.analyze_with_auto_fallback(None, summary_prompt, model_id)
        
        return {
            "model_used": model_info['name'],
            "model_id": model_id,
            "provider_used": provider_used,
            "executive_summary": summary,
            **results
        }
    
    def format_for_rag(self, analysis_results: Dict) -> str:
        """Format for RAG indexing"""
        provider_note = f" via {analysis_results.get('provider_used', 'unknown')}"
        
        return f"""CAD DRAWING ANALYSIS (Analyzed by {analysis_results['model_used']}{provider_note})

EXECUTIVE SUMMARY:
{analysis_results['executive_summary']}

OVERVIEW ANALYSIS:
{analysis_results['stage_1_overview']}

TECHNICAL DETAILS:
{analysis_results['stage_2_technical']}

COMPONENTS & FEATURES:
{analysis_results['stage_3_components']}

MEASUREMENTS & DIMENSIONS:
{analysis_results['stage_4_measurements']}

QUALITY ASSESSMENT:
{analysis_results['stage_5_quality']}
"""

# Custom exception for quota errors
class QuotaExceededError(Exception):
    """Raised when API quota is exceeded"""
    pass
